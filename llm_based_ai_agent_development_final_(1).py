# -*- coding: utf-8 -*-
"""LLM_Based_AI_Agent_Development_Final (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yt3DDSFRdAWKnfZlk_LW0qnMK5Q8aZam
"""

# Colab Setup
!pip install -q \
    langchain \
    langchain-community \
    langchain-huggingface \
    langgraph \
    chromadb \
    sentence-transformers \
    transformers \
    accelerate \
    pypdf \
    fastapi \
    uvicorn \
    requests

!pip install pdfplumber

"""### Data Preprocessing"""

import pdfplumber
from pathlib import Path

PDF_PATH = "/content/EAST WEST UNIVERSITY DISCIPLINARY.pdf"

def extract_pdf_text(pdf_path):
    pages = []
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                pages.append({
                    "page_number": i + 1,
                    "text": text.strip()
                })
    return pages

raw_pages = extract_pdf_text(PDF_PATH)

print(f"Extracted {len(raw_pages)} pages")

import re

def clean_text(text):
    text = re.sub(r"Page\s+\d+\s+of\s+\d+", "", text)
    text = re.sub(r"Tuesday,\s+\w+\s+\d{1,2},\s+\d{4}", "", text)
    text = re.sub(r"\n{2,}", "\n", text)
    return text.strip()

for page in raw_pages:
    page["text"] = clean_text(page["text"])

full_text = "\n\n".join(page["text"] for page in raw_pages)

SECTION_PATTERN = re.compile(
    r"\n(?=\d+\.\s+[A-Z][A-Z\s\-&]+)",
    re.MULTILINE
)

raw_sections = SECTION_PATTERN.split("\n" + full_text)

def parse_section(section_text):
    lines = section_text.strip().split("\n", 1)

    header = lines[0].strip()
    body = lines[1].strip() if len(lines) > 1 else ""

    match = re.match(r"(\d+)\.\s+(.*)", header)
    if not match:
        return None

    section_number = match.group(1)
    section_title = match.group(2)

    return {
        "section_number": section_number,
        "section_title": section_title,
        "content": body
    }

sections = []
for sec in raw_sections:
    parsed = parse_section(sec)
    if parsed:
        sections.append(parsed)

print(f"Parsed {len(sections)} sections")

CLAUSE_PATTERN = re.compile(r"(?=\d+\.\d+(?:\.\d+)*)")

def split_clauses(section):
    # Safely get content
    text = section.get("content", "").strip()

    # If no body text, return section with empty clauses
    if not text:
        section["clauses"] = []
        section.pop("content", None)
        return section

    chunks = CLAUSE_PATTERN.split(text)

    clauses = []
    for chunk in chunks:
        chunk = chunk.strip()
        if not chunk:
            continue

        match = re.match(r"(\d+\.\d+(?:\.\d+)*)\s+(.*)", chunk, re.DOTALL)
        if match:
            clauses.append({
                "clause_id": match.group(1),
                "text": match.group(2).strip()
            })

    section["clauses"] = clauses
    section.pop("content", None)
    return section

structured_sections = [split_clauses(sec) for sec in sections]

import json

final_json = {
    "metadata": {
        "title": "The East West University Disciplinary Code for Students, 2011",
        "source": Path(PDF_PATH).name,
        "total_pages": len(raw_pages)
    },
    "sections": structured_sections
}

with open("ewu_disciplinary_code.json", "w", encoding="utf-8") as f:
    json.dump(final_json, f, indent=2, ensure_ascii=False)

print("JSON saved as ewu_disciplinary_code.json")

"""### Chunking and Embedding"""

from langchain_core.documents import Document
import json

with open("ewu_disciplinary_code.json", "r", encoding="utf-8") as f:
    ewu_data = json.load(f)

documents = []

for section in ewu_data["sections"]:
    section_no = section["section_number"]
    section_title = section["section_title"]

    for clause in section.get("clauses", []):
        documents.append(
            Document(
                page_content=clause["text"],
                metadata={
                    "section": section_no,
                    "section_title": section_title,
                    "clause_id": clause["clause_id"],
                    "source": "EWU Disciplinary Code"
                }
            )
        )

print(f"Total documents created: {len(documents)}")

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=80
)

chunks = text_splitter.split_documents(documents)

print("Total chunks:", len(chunks))

from langchain_huggingface.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ChromaDB Vector Store
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="./chroma_ewu_disciplinary"
)

vectorstore.persist()
print("ChromaDB for EWU Disciplinary Code created.")

"""### Load Model"""

# Login to hugging face (if use licenced model, need to authenticate the token)
from huggingface_hub import login
login()
# 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 -> Token

# Load Meta-Llama-3-8B
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_id = "google/gemma-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    use_auth_token=True
)

llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id

)

# RAG Answer Generator
def rag_answer(question, context):
    prompt = f"""
You are answering questions about the East West University Disciplinary Code.

Answer STRICTLY using the context.
If the answer is not explicitly stated, say:
"I don't know based on the provided documents."

Context:
{context}

Question:
{question}

Answer:
"""
    result = llm_pipeline(prompt)
    return result[0]["generated_text"].split("Answer:")[-1].strip()

"""### Retriever"""

def retrieve_docs(state):
    query = state["query"]
    docs = vectorstore.similarity_search(query, k=4)

    state["context"] = "\n\n".join(
        f"[Section {d.metadata['section']} | Clause {d.metadata['clause_id']}]\n{d.page_content}"
        for d in docs
    )
    return state

def planner(state):
    state["action"] = "answer"
    return state


def answer_node(state):
    answer = rag_answer(state["query"], state["context"])
    state["final_answer"] = answer
    return state


from langgraph.graph import StateGraph

graph = StateGraph(dict)

graph.add_node("planner", planner)
graph.add_node("retriever", retrieve_docs)
graph.add_node("answer", answer_node)

graph.set_entry_point("planner")
graph.add_edge("planner", "retriever")
graph.add_edge("retriever", "answer")
graph.set_finish_point("answer")

agent = graph.compile()

query = "What are “unfair or illegal means” in examinations according to the Code?"

result = agent.invoke({"query": query})

print(result["final_answer"])

"""### Testing Queries
Test your system with this queries:
1. What actions are considered student misconduct under the EWU Disciplinary Code?
2. What are “unfair or illegal means” in examinations according to the Code?
3. What punishments can be awarded if a student is found guilty of misconduct?
4. Who are the members of the Disciplinary Committee and what is its authority?
5. What is the procedure after a complaint is lodged against a student?
6. Can a student be suspended before the disciplinary hearing? If yes, under what conditions?
7. What rights does an accused student have during a disciplinary hearing?
8. How does the university handle cases related to sexual harassment?
9. Can a student appeal a disciplinary decision, and what is the appeal process?
10. What immediate powers does an invigilator have if unfair means are detected during an examination?

# Task
Create a Gradio application that takes a user query as input, uses the LangGraph agent to process it, and then displays the original query, the context retrieved by the retriever node, and the final answer generated by the answer node. Finally, test the Gradio UI with various queries.
"""

!pip install -q gradio

def rag_interface(user_query):
    result = agent.invoke({"query": user_query})

    original_query = result.get("query", "N/A")
    retrieved_context = result.get("context", "No context retrieved.")
    final_answer = result.get("final_answer", "No answer generated.")

    formatted_output = f"""
Original Query: {original_query}

Retrieved Context:
{retrieved_context}

Final Answer: {final_answer}
"""
    return formatted_output

print("rag_interface function defined.")

"""## Launch Gradio Application"""

import gradio as gr

iface = gr.Interface(
    fn=rag_interface,
    inputs=gr.Textbox(label='Enter your query:', placeholder='What actions are considered student misconduct?'),
    outputs=gr.Markdown(label='RAG Agent Output'),
    title='EWU Disciplinary Agent'
)

iface.launch(share=True)